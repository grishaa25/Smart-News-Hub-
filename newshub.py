# -*- coding: utf-8 -*-
"""NEWSHUB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115Z_S1-uxMrg3xNDbCG9bVNP41Bu84OQ
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pip -q install langchain
!pip -q install langchain_cohere
!pip -q install langchain_community
!pip -q install google-search-results
!pip -q install faiss-cpu
!pip -q install chromadb
!pip -q install langchain-openai
!pip -q install upstash_redis
!pip install -U duckduckgo-search
!pip install -qU\
pinecone-client==3.2.2\
pandas==2.0.3
!pip install pinecone-client[grpc]
!pip install langchain_pinecone
!pip install lark-parser
# %pip install --upgrade --quiet  lark
!pip install lark
!pip install googlemaps
!pip install -U langchain langchain-openai
!pip install Cohere
!pip install transformers accelerate
!pip install langsmith
!pip install langchain_openai langchain_core
!pip install langgraph
!capture --no-stderr
!pip install -U langgraph langsmith

pip install --upgrade pinecone-client

# First, import pandas library
import pandas as pd

# Load the preprocessed dataset (replace with the path to your preprocessed dataset in Google Drive)
data_path = '/content/drive/MyDrive/india-news-headlines (1)_processed (1).csv'
df = pd.read_csv(data_path)

# Print the column names to inspect the dataset
print("Columns in the dataset:", df.columns)

# Display the first few rows of the dataset to further understand its structure
df.head()
df.shape
()

# Install necessary libraries
!pip install transformers
!pip install rouge_score
!pip install torch
!pip install spacy
!python -m spacy download en_core_web_sm

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.agents import AgentExecutor, create_openai_tools_agent, create_tool_calling_agent
from langchain_core.messages import HumanMessage, AIMessage
from langchain.tools.retriever import create_retriever_tool
from langchain.chains import create_retrieval_chain
from langchain.chains import LLMChain
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.vectorstores import Chroma, FAISS
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.agents import Tool
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories.upstash_redis import (
    UpstashRedisChatMessageHistory,
)
from langchain.vectorstores import Pinecone
import torch
import gc
import torch.nn.functional as F
from torch import Tensor
import pinecone as pc
import time

# Mount Google Drive to access the datasets
from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Load the preprocessed dataset (replace with the path to your preprocessed dataset in Google Drive)
data_path = '/content/drive/MyDrive/india-news-headlines (1)_processed (1).csv'
df = pd.read_csv(data_path)

# Show the first few rows to understand the structure of the dataset
print(df.head())  # To inspect and find the column containing text

# You can also load the keywords CSV (this is optional for summarization)
keywords_path = '/content/drive/MyDrive/keywords.csv'
keywords_df = pd.read_csv(keywords_path)
keywords_df.head()

# Import necessary libraries for summarization
import spacy
from transformers import pipeline
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load SpaCy model for text processing
nlp = spacy.load("en_core_web_sm")

# Load the smaller BART model (faster execution)
summarizer = pipeline("summarization", model="facebook/bart-base")

# Function to clean and preprocess the text (removes stopwords and punctuation)
def clean_text(text):
    doc = nlp(text)
    return [token.text.lower() for token in doc if token.text.lower() not in spacy.lang.en.stop_words.STOP_WORDS and not token.is_punct()]

import getpass
import os

if not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")

# Commented out IPython magic to ensure Python compatibility.
!pip install -U langchain-community
!pip install -qU \
pinecone-client==3.2.2 \
pandas==2.0.3
!pip install pinecone-client[grpc]
!pip install langchain_pinecone
!pip install -U langchain_openai

from pinecone import Pinecone
import os
import openai
import time
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
# %pip install --upgrade --quiet spacy
from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings
from langchain_openai import OpenAIEmbeddings


# Initialize Pinecone
api_key = "pcsk_876wa_QAPexWdJB47XT1AAs55Cs8tywwaZaCVYPVSkRGX3f192x9rnMJVek2cNFune4mm"
index_name = "newshub"
pc = Pinecone(api_key="pcsk_876wa_QAPexWdJB47XT1AAs55Cs8tywwaZaCVYPVSkRGX3f192x9rnMJVek2cNFune4mm")

host = "https://newshub-5e9da55.svc.aped-4627-b74a.pinecone.io"
index = pc.Index(index_name, dimension=1536)

# Load and split documents
loader = CSVLoader(file_path='/content/drive/MyDrive/india-news-headlines (1)_processed (1).csv')
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
splitDocs = splitter.split_documents(docs)

time.sleep(1)

# Initialize the OpenAI Embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# Define the embedding function with retries
def get_batch_embeddings_with_retries(texts, max_retries=5):
    retries = 0
    while retries <= max_retries:
        try:
            result = embeddings.embed_documents(texts)
            return result
        except Exception as e:
            print(f"Error occurred: {e}. Retrying...")
            retries += 1
            time.sleep(1)  # Wait for a bit before retrying
    raise Exception("Failed to get embeddings after multiple retries.")

def get_existing_vector_ids(index, namespace):
    try:
        response = index.query("", top_k=0, namespace=namespace)
        existing_ids = response["ids"]
        return set(existing_ids)
    except Exception as e:
        print(f"Exception occurred while fetching existing vector IDs: {e}")
        return set()

# Embed and upsert documents in batches
batch_size = 200  # Increased batch size
namespace = "news_data"  # Add namespace variable if not already defined
for i in range(0, len(splitDocs), batch_size):
    batch_texts = [doc.page_content for doc in splitDocs[i:i + batch_size]]

    # Get embeddings for the current batch
    batch_embeddings = get_batch_embeddings_with_retries(batch_texts)

    # Get existing vector IDs in the namespace
    existing_vector_ids = get_existing_vector_ids(index, namespace)

    # Prepare data for upsert, ensuring no duplicates
    to_upsert = []
    for j, vector in enumerate(batch_embeddings):
        vector_id = f"{i * batch_size + j}"
        if vector_id not in existing_vector_ids:
            # Include the text content in the metadata
            metadata = {'text': batch_texts[j]}
            # Prepare the data for upsert with metadata
            to_upsert.append((vector_id, vector, metadata))

    # Ensure the upsert only happens once per batch
    if to_upsert:
        try:
            # Upsert embeddings into Pinecone index under the specified namespace
            index.upsert(vectors=to_upsert, namespace=namespace)
        except Exception as e:
            print(f"Exception occurred during upsert: {e}")

        # Sleep to avoid hitting rate limits
        time.sleep(1)

"""!pip install -U langchain-community
!pip install -qU\
pinecone-client==3.2.2\
pandas==2.0.3
!pip install pinecone-client[grpc]
!pip install langchain_pinecone
!pip install -U langchain_openai

from pinecone import Pinecone
import os
import openai
import time
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
%pip install --upgrade --quiet  spacy
from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings
from langchain_openai import OpenAIEmbeddings


# Initialize Pinecone
api_key = "pcsk_4gAKmd_2238zt6QjbeVrc8wBRi4ae6Pa5KtYoGWytqP3PUGDzKg7UYgVS8dV7fpQLYYJBk"
index_name = "newshub"
pc = Pinecone(api_key="pcsk_4gAKmd_2238zt6QjbeVrc8wBRi4ae6Pa5KtYoGWytqP3PUGDzKg7UYgVS8dV7fpQLYYJBk")

host = "https://newshub-lqkgkjr.svc.aped-4627-b74a.pinecone.io"
index = pc.Index(index_name, dimension = 1536)

# Load and split documents
loader = CSVLoader(file_path='/content/drive/MyDrive/india-news-headlines (1)_processed (1).csv')
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
splitDocs = splitter.split_documents(docs)

time.sleep(1)

# Initialize the OpenAI Embeddings


embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small")
# Define the embedding function with retries
def get_batch_embeddings_with_retries(texts, max_retries=5):
    retries = 0
    while retries <= max_retries:
        try:
            result = embeddings.embed_documents(texts)
            return result
        except Exception as e:
            print(f"Error occurred: {e}. Retrying...")
            retries += 1
            time.sleep(1)  # Wait for a bit before retrying
    raise Exception("Failed to get embeddings after multiple retries.")

def get_existing_vector_ids(index, namespace):
    try:
        response = index.query("", top_k=0, namespace=namespace)
        existing_ids = response["ids"]
        return set(existing_ids)
    except Exception as e:
        print(f"Exception occurred while fetching existing vector IDs: {e}")
        return set()


# Embed and upsert documents in batches
batch_size = 200  # Increased batch size
for i in range(0, len(splitDocs), batch_size):
    batch_texts = [doc.page_content for doc in splitDocs[i:i+batch_size]]


    # Get embeddings for the current batch
    batch_embeddings = get_batch_embeddings_with_retries(batch_texts)

    # Get existing vector IDs in the namespace
    existing_vector_ids = get_existing_vector_ids(index, namespace)

    # Prepare data for upsert, ensuring no duplicates
    to_upsert = []
    for j, vector in enumerate(batch_embeddings):
        vector_id = f"{i * batch_size + j}"
        if vector_id not in existing_vector_ids:
            
    to_upsert.append((vector_id, vector))

    if to_upsert:
        try:
            # Upsert embeddings into Pinecone index under the specified namespace
            index.upsert(vectors=to_upsert, namespace=namespace)
        except Exception as e:
            print(f"Exception occurred during upsert: {e}")

    # Sleep to avoid hitting rate limits
    time.sleep(1)

"""

# the prompt template
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
prompt_main = ChatPromptTemplate.from_messages([
    ("system", """
System Role:
You are an intelligent assistant tasked with retrieving, summarizing, and presenting information from a large database of articles. Your goal is to provide concise, accurate, and actionable outputs to save the user's time and enhance their experience.

...

Step 1: Retrieve Relevant Articles
Extract keywords from the user's query and search the database for relevant articles. Prioritize recent, reliable, and highly relevant sources.

Step 2: Summarize Key Points
Generate concise summaries of the retrieved articles. Highlight the most critical insights, findings, or updates related to the query. Each summary should be no longer than 3 sentences.

Step 3: Present Results
Deliver the following structured output for each article:

Article Title: [Title]
Source: [Source Name], [Publication Date]
Summary: [Concise Summary]
Keywords Extracted: [Key Terms]
Direct Link (if applicable): [URL]
...
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

from langchain.chains import RetrievalQAWithSourcesChain
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.llms import Cohere
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_core.pydantic_v1 import BaseModel, Field
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore

# Initialize Pinecone
api_key = "pcsk_876wa_QAPexWdJB47XT1AAs55Cs8tywwaZaCVYPVSkRGX3f192x9rnMJVek2cNFune4mm"
index_name = "newshub"
pc = Pinecone(api_key="pcsk_876wa_QAPexWdJB47XT1AAs55Cs8tywwaZaCVYPVSkRGX3f192x9rnMJVek2cNFune4mm")


host = "https://newshub-5e9da55.svc.aped-4627-b74a.pinecone.io"
index = pc.Index(index_name, dimension=1536)


embeddings = OpenAIEmbeddings( model="text-embedding-3-small")
vectorstore = PineconeVectorStore(index =index,namespace= "news_data" ,embedding=embeddings,text_key = 'text')

retriever = vectorstore.as_retriever()

retriever.invoke('articles of 2020')

def retrieve_documents(query: str):
    documents = retriever.get_relevant_documents(query)
    return [doc.page_content for doc in documents]

retrieval_tool = Tool(
    name="RetrievalTool",
    func=retrieve_documents,
    description="Retrieves and re-ranks documents based on the input query."
)

tools = [retrieval_tool]

from langchain.agents import initialize_agent
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

agent = create_tool_calling_agent(
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0),
    prompt=prompt_main,
    tools=tools,
)


agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,
    handle_parsing_errors=True,
    memory=memory)

from langchain_community.callbacks import get_openai_callback

if __name__ == '__main__':
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break

        try:
            response = agent_executor.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": "<foo>"}}
            )
            print("")
            print("Assistant:", response.get('output', 'No answer found'))
            print("")
        except ValueError as e:
            print(f"Error: {e}")
            print("Please try again.")